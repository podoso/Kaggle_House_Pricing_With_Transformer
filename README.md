## Description
The main objective of this project is to learn the Transformer framework through the house price prediction competition on Kaggle.

First, during the data [preprocessing stage](https://zh.d2l.ai/chapter_multilayer-perceptrons/kaggle-house-price.html "Data preprocessing:"), the data is transformed into a vector of length 330. Since the data labels do not contain obvious sequential information, the Positional Encoding part of the original architecture was removed. To ensure that the vector length is divisible by the number of Attention Heads, the number of heads in this experiment is set to 6. The settings for the Encoder part remain the same as in the original experiment, with the number of layers kept at 6. The modification in the Decoder part involves the target vector input. As the Transformer framework is a Seq2Seq model primarily used for NLP tasks, the final output is generally a sequence. However, the output for this task is a single numeric value, so during the training phase, the final label is stretched into a vector of length 330. During training, due to the large number of parameters in the model, a relatively high learning rate, such as around 0.5, is set to accelerate the convergence speed of the model. The original code is based on the Transformer tutorial by [Datawhale](https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs "Datawhale tutorial:").
![The Architecture of the model](https://github.com/podoso/Kaggle_House_Pricing_With_Transformer/blob/main/My_Transformer.png)
